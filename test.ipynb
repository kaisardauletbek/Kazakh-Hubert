{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "import soundfile as sf\n",
    "import transformers\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Tokenizer, Wav2Vec2Processor\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "import traceback\n",
    "from transformers import AutoProcessor\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized processor in facebook/hubert-large-ls960-ft. Should have a `processor_type` key in its preprocessor_config.json, or one of the following `model_type` keys in its config.json: clip, flava, layoutlmv2, layoutlmv3, layoutxlm, sew, sew-d, speech_to_text, speech_to_text_2, trocr, unispeech, unispeech-sat, vilt, vision-text-dual-encoder, wav2vec2, wav2vec2-conformer, wav2vec2_with_lm, wavlm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/raid/kaisar_dauletbek/train_hubert/test.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote_server_17/raid/kaisar_dauletbek/train_hubert/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# # Define the feature extractor and tokenizer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote_server_17/raid/kaisar_dauletbek/train_hubert/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote_server_17/raid/kaisar_dauletbek/train_hubert/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote_server_17/raid/kaisar_dauletbek/train_hubert/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote_server_17/raid/kaisar_dauletbek/train_hubert/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# # Define the processor\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote_server_17/raid/kaisar_dauletbek/train_hubert/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bremote_server_17/raid/kaisar_dauletbek/train_hubert/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/hubert-large-ls960-ft\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py:249\u001b[0m, in \u001b[0;36mAutoProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m PROCESSOR_MAPPING:\n\u001b[1;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m PROCESSOR_MAPPING[\u001b[39mtype\u001b[39m(config)]\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 249\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    250\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized processor in \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m. Should have a `processor_type` key in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mits \u001b[39m\u001b[39m{\u001b[39;00mFEATURE_EXTRACTOR_NAME\u001b[39m}\u001b[39;00m\u001b[39m, or one of the following `model_type` keys in its \u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m PROCESSOR_MAPPING_NAMES\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized processor in facebook/hubert-large-ls960-ft. Should have a `processor_type` key in its preprocessor_config.json, or one of the following `model_type` keys in its config.json: clip, flava, layoutlmv2, layoutlmv3, layoutxlm, sew, sew-d, speech_to_text, speech_to_text_2, trocr, unispeech, unispeech-sat, vilt, vision-text-dual-encoder, wav2vec2, wav2vec2-conformer, wav2vec2_with_lm, wavlm"
     ]
    }
   ],
   "source": [
    "# # Define the feature extractor and tokenizer\n",
    "# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "# tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# # Define the processor\n",
    "# processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(root_dir):\n",
    "    data = {\"path\": [], \"transcript\": [], \"split\": []}\n",
    "\n",
    "    # For each subdirectory\n",
    "    for split in [\"Train\", \"Dev\", \"Test\"]:\n",
    "        split_dir = os.path.join(root_dir, split)\n",
    "        for category in os.listdir(split_dir):\n",
    "            category_dir = os.path.join(split_dir, category)\n",
    "\n",
    "            # For each file\n",
    "            for file in os.listdir(category_dir):\n",
    "                if file.endswith(\".flac\"):\n",
    "                    path = os.path.join(category_dir, file)\n",
    "                    transcript_path = path.replace(\".flac\", \".txt\")\n",
    "\n",
    "                    with open(transcript_path, \"r\") as f:\n",
    "                        transcript = f.read().strip()\n",
    "\n",
    "                    data[\"path\"].append(path)\n",
    "                    data[\"transcript\"].append(transcript)\n",
    "                    data[\"split\"].append(split)\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "def map_to_array(batch):\n",
    "    speech, _ = sf.read(batch[\"path\"])\n",
    "    batch[\"speech\"] = speech\n",
    "    return batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7faf7a35beb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaisar_dauletbek/anaconda3/envs/nlp/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset = load_data(\"/raid/kaisar_dauletbek/datasets/ISSAI_KSC2\")\n",
    "\n",
    "# Set the number of processes to use for parallel processing\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Parallelize the mapping operation across CPUs\n",
    "dataset = dataset.map(map_to_array, num_proc=num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.17k/5.17k [00:00<00:00, 6.23MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset librispeech_asr_dummy/clean to /home/kaisar_dauletbek/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 9.08M/9.08M [00:03<00:00, 2.84MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:04<00:00,  4.66s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 22.62it/s]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset librispeech_asr_dummy downloaded and prepared to /home/kaisar_dauletbek/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "ds = ds.map(map_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id', 'speech'],\n",
       "    num_rows: 73\n",
       "})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "myenv"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
